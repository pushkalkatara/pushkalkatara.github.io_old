<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hi! Pushkal here.</title>
    <description>CS Undergrad interested in Robotics, Deep Learning and Artificial Intelligence</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 18 Sep 2019 19:13:37 +0530</pubDate>
    <lastBuildDate>Wed, 18 Sep 2019 19:13:37 +0530</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>URSim - Open Source Unmanned Underwater Vehicles Simulator</title>
        <description>&lt;p&gt;URSim is a 3D underwater simulation framework for Unmanned Underwater Vehicles (UUVs) developed using ROS and Unity3D. Worked in collaboration with team SRMAUV for AUV’s simulation and published as a paper in 2019 IEEE Symposium on Underwater Technology. 2019/04/16-19 @NSYSU, Kaohsiung, Taiwan. &lt;br /&gt;
&lt;strong&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/8734309&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/srmauvsoftware/URSim&quot;&gt;Code&lt;/a&gt; | &lt;a href=&quot;https://docs.google.com/presentation/d/1ChLeVlUOYZ9LOEp-kk0C3HupoHN2Ctm8oyvjrr45BvY/edit?usp=sharing&quot;&gt;Presentation&lt;/a&gt;&lt;/strong&gt;
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;It is capable of simulating feedback control systems, dynamic model, underwater vision and mission planning for underwater vehicles. The simulation provides support for underwater sensor modules, underwater physics, collision kinematics and is highly configurable to simulate a realistic underwater environment. The software architecture is adaptive to algorithms for control systems, image processing, navigation and manipulation.&lt;/p&gt;
</description>
        <pubDate>Sat, 20 Apr 2019 13:06:00 +0530</pubDate>
        <link>http://localhost:4000/post/2019/04/20/ursim.html</link>
        <guid isPermaLink="true">http://localhost:4000/post/2019/04/20/ursim.html</guid>
        
        <category>ROS</category>
        
        <category>Unity3D</category>
        
        <category>Blender</category>
        
        <category>underwater-simulation</category>
        
        <category>sensor-simulation</category>
        
        <category>SRMAUV</category>
        
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>darket_ros_v3</title>
        <description>&lt;p&gt;Robotics Operating System package developed for object detection and tracking on ROS image topics. Object detection based on Yolo v3 and tracking based on Kalman Filter and Optical flow. Implemented multi-threading and thread synchronization techniques with fetch, detect, publish threads to capture, infer and publish results as ROS topics. Solved the issue of memory leak on Jetson TX1 on leggedrobotics repository. Deployed on Jetson TX1 with inference of 4 FPS. I built this as part of SRM Autonomous Underwater Vehicle stack. &lt;strong&gt;&lt;a href=&quot;https://github.com/pushkalkatara/darknet_ros&quot;&gt;Source Code&lt;/a&gt; | &lt;a href=&quot;https://drive.google.com/open?id=1-1htIKVxDWda37Z7q-qvudrceZefzoxt&quot;&gt;Live BUOY Underwater Detection Video&lt;/a&gt;&lt;/strong&gt;
&lt;!--more--&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 08 Dec 2018 13:06:00 +0530</pubDate>
        <link>http://localhost:4000/post/2018/12/08/darket-ros.html</link>
        <guid isPermaLink="true">http://localhost:4000/post/2018/12/08/darket-ros.html</guid>
        
        <category>yolo</category>
        
        <category>alexeyab-darknet</category>
        
        <category>deep-learning</category>
        
        <category>computer-vision</category>
        
        <category>object-detection</category>
        
        <category>darknet-ros</category>
        
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Improving VisualStates@JdeRobot - Google Summer Of Code 2018</title>
        <description>&lt;p&gt;Worked with organization JdeRobot on VisualStates, developing a tool for programming robot behaviors using Finite State Machines. It generates ROS node as output, in C++ or Python, which connects to the configured robot drivers at runtime.  &lt;br /&gt;
&lt;strong&gt;&lt;a href=&quot;https://github.com/JdeRobot/VisualStates/pull/59&quot;&gt;GSoC Github PR&lt;/a&gt; |
&lt;a href=&quot;https://blog.usejournal.com/google-summer-of-code-2018-2c970dcd9647&quot;&gt;Noteworthy Blog&lt;/a&gt; |
&lt;a href=&quot;https://summerofcode.withgoogle.com/archive/2018/projects/6028761103335424/&quot;&gt;Project Page&lt;/a&gt; | 
&lt;a href=&quot;https://www.youtube.com/watch?v=1iYlJLJkESU&amp;amp;t=6s&quot;&gt;Prius Autonomous Behavior&lt;/a&gt;&lt;/strong&gt;
&lt;!--more--&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Implemented recursive automata import functionality and global-local namespace as new features to the tool. Wrote documentation, articles and worked in collaboration with mentors through weekly video sessions and daily emails. Developed simulation examples in Gazebo simulator using computer vision and sensor manipulation algorithms using Toyota Prius Car for autonomous navigation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More details coming soon :D&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 08 Aug 2018 13:06:00 +0530</pubDate>
        <link>http://localhost:4000/post/2018/08/08/gsoc.html</link>
        <guid isPermaLink="true">http://localhost:4000/post/2018/08/08/gsoc.html</guid>
        
        <category>GSoC</category>
        
        <category>VisualStates</category>
        
        <category>JdeRobot</category>
        
        <category>robotics</category>
        
        <category>computer-vision</category>
        
        <category>ros</category>
        
        <category>gazebo</category>
        
        <category>simulation</category>
        
        <category>prius</category>
        
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Graphical User Interface for UUVs</title>
        <description>&lt;p&gt;Visualizing Telemetry Information, Sensor data, mission status with feedback and controller’s dynamics is important for an Unmanned Underwater Vehicles debugging. For the purpose a fast implementation of GUI is important. &lt;a href=&quot;https://wiki.python.org/moin/TkInter&quot;&gt;Tkinter&lt;/a&gt;, a python library provides easy functions to implement graphical user interfaces. It is a thin object-oriented layer on top of Tcl/Tk.
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;Main issue was to integrate information from ROS(Robotics Operating System) messages, services, actions with GUI using Tkinter. As Tkinter runs its own loop(root.mainloop()) and ROS has own loop (rospy.spin()). Conflicting these two loops, getting information from ROS Subscriber callbacks, sending actions as actionlib client was a major issue. But while implementing, ROS accepted Tkinter’s mainloop as its own rospy.spin() to update the Subscribers and call the callback functions. Hence, Subscribing to a topic in init of a class and creating its callback to perform GUI operations worked out.&lt;br /&gt;
Also several tasks like integrating actionlib into GUI requires threading as sending a goal and waiting for the goal may stall the GUI’s main loop.&lt;/p&gt;

&lt;p&gt;The whole implementation is a flexible Object oriented approach as to create modules(grids in Tkinter) of classes predefined of particular ROS messages.s A structure of a grid in the GUI to display the message is also defined in the class. Creating objects of these classes and stacking on rows and columns as arguments in the main GUI is possible for flexibility.&lt;/p&gt;

&lt;p&gt;Currently GUI supports 3 frames:&lt;/p&gt;

&lt;h2 id=&quot;telemetry-ui&quot;&gt;Telemetry UI&lt;/h2&gt;
&lt;p&gt;Frame is used to visualize all sensor information available from the AUV. It subscribes to all the topics realted to sensor data like, /imu/data, /pressure etc. The main tkinter loop spins and calls the callback functions of all the subscribers to update the data from corresponding topic.
&lt;img src=&quot;/assets/images/gui/telemetry.gif&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;controller-ui&quot;&gt;Controller UI&lt;/h2&gt;
&lt;p&gt;Frame is used while testing the AUV, it has integrated Actionlib clients, where a goal is received from the GUI and sent to the server. Also dynamically PID values can be altered. Integrating Actionlib required threading as the whole task of actionlib cannot be performed in the Tkinter’s main loop.
&lt;img src=&quot;/assets/images/gui/actionlib.gif&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;image-processing-suite&quot;&gt;Image Processing Suite&lt;/h2&gt;
&lt;p&gt;Frame is used to hardcode HSV Values to detect underwater objects like Colored Buoys and colored pipe of gates underwater. Dynamically HSV values can be altered and tuned and verified in the Processed Image through GUI. It subscribes to the Image processing script running and dynamically alters the HSV values through dynamic reconfigure server running on a alternate thread. Alternatively we can create a topic too which consists of HSV Values and publish when slider is changed. As ROS sends images in the form of Image Message, cv_bridge package converts the Ros Image message to OpenCV compatible image. But Tkinter also doesnt support OpenCV numpy arrays to display as image. Workaround over this was using PIL library to display image in a Tkinter label.&lt;/p&gt;
</description>
        <pubDate>Thu, 11 Jan 2018 04:59:00 +0530</pubDate>
        <link>http://localhost:4000/blog/2018/01/11/gui.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2018/01/11/gui.html</guid>
        
        <category>Mission Planner</category>
        
        <category>SMACH</category>
        
        <category>SRMAUV</category>
        
        <category>Actionlib</category>
        
        <category>GUI</category>
        
        <category>Telemetry</category>
        
        <category>Tkinter</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>AUV Mission Planning using State Machines</title>
        <description>&lt;p&gt;Robotics Operating System provides SMACH package which is useful when you want a robot to execute some complex plan, where all possible states and state transitions can be described explicitly. This basically takes the hacking out of hacking together different modules to make robotics systems perform specific complex tasks with state management.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fast prototyping: The straightforward Python-based SMACH syntax makes it easy to quickly prototype a state machine and start running it.&lt;/li&gt;
  &lt;li&gt;Complex state machines: SMACH allows you to design, maintain and debug large, complex hierarchical state machines.&lt;/li&gt;
  &lt;li&gt;Introspection: SMACH gives you full introspection in your state machines, state transitions, data flow, etc.
&lt;!--more--&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;application&quot;&gt;Application:&lt;/h3&gt;
&lt;p&gt;Consider a situation of an AUV which needs to perform tasks underwater autonomously. An autonomous system completely reply on sensor data and as the sensors are noisy, there are always chances of a system to fail. Actions performed underwater by the vehicle should be considered as an state and the state transitions must be logged for further diagonosis. Also to perform specific tasks underwater, states need to be pre-arraged in a mission planner to search, attempt and preempt the task in dynamic conditions. It is basically a robust implementation of decision rules with specified conditions.&lt;/p&gt;

&lt;h3 id=&quot;documentation&quot;&gt;Documentation:&lt;/h3&gt;
&lt;p&gt;Smach has containers which contains states with defined state outcomes. Each container and state is declared on construction with defined transition outcomes. A state might provide outcomes such as ‘succeeded’, ‘aborted’, or ‘preempted’. These outcomes would be associated with other states, thus forming a transition. Also Smach provides interface to pass data between different states using Input and Output Keys. Also, The State base class includes an interface for coordinating preempt requests between containers and contained states. Further, we can visualize the states and containers with user data using the SMACH Viewer.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;inference&quot;&gt;Inference&lt;/h3&gt;
&lt;p&gt;I implemented smach in an Autonomous Underwater Vehicle -&lt;br /&gt;
Initial task of an underwater vehicle is to sink underwater i.e. go to a certain depth underwater. Using the DepthActionClient, we can feed a desired depth as a target and the DepthActionServer would try to achieve this depth. But sometimes due to external factors, an AUV may deroute not completeting the DepthGoal. As it as an autonomous system, it needs to be aware of its condition inside. Hence, a SimpleStateMachine Container comes in play. SMACH provides &lt;a href=&quot;http://wiki.ros.org/smach/Tutorials/Simple%20Action%20State&quot;&gt;SimpleActionState&lt;/a&gt; that call directly into the actionlib interface.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Create a StateMachine (‘sink_state’) and assign outcomes [‘sink’, ‘not-sink’]&lt;/li&gt;
  &lt;li&gt;Create a Callback to check the output received from Actionlib and SimpleActionState to send the goal, further decide the outcome of the state as sink or not sink.&lt;br /&gt;
&lt;img src=&quot;/assets/images/mission_planner/alpheus_state.png&quot; alt=&quot;Introspection&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Looking into further states, the complexitiy of mission planner increases. Consider a situtation in which an AUV needs to turn at an angle 30 degrees with a depth goal of extra 10m deep. These are two parallel states which needs to be achieved using the DepthActionClient and DepthActionServer. SMACH provides Concurrent State Machines for implementing parallel states.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Create 2 state machines classes related to depth and heading control.&lt;/li&gt;
  &lt;li&gt;Create a callback to check the output recevied from Actionlib and SimpleActionState to send the goal, further callback decides the next state accroding to the reply from actionlib.
&lt;img src=&quot;/assets/images/mission_planner/smacch.png&quot; alt=&quot;Introspection&quot; /&gt;&lt;br /&gt;
Similarly all the tasks to be performed can be implemented in Hierarchical, Concurrent, Sequence or Iterative methods using Smach library.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is a simulation of mission planner tests-
&lt;img src=&quot;/assets/images/mission_planner/mission.gif&quot; alt=&quot;MissionPlannner&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 06 Jan 2018 04:59:00 +0530</pubDate>
        <link>http://localhost:4000/blog/2018/01/06/mp.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2018/01/06/mp.html</guid>
        
        <category>Mission Planner</category>
        
        <category>SMACH</category>
        
        <category>SRMAUV</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Actionlib in AUV Software Architecture</title>
        <description>&lt;p&gt;Robotics Operating System provides actionlib package which is a robust implementation to perfrom tasks in a robot, I implemented Actionlib in Autonomous Underwater Vehicle’s software stack to perfrom goals under water with logging states.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id=&quot;actionlib-usecase&quot;&gt;Actionlib Usecase&lt;/h2&gt;
&lt;p&gt;To perform particular tasks in robots we have cases which include sending a request to a node, and also receive a reply to the request.
We can achieve this using ROS Services but in some cases if the service takes a long time to execute, the user might want the
ability to cancel the request during execution or get periodic feedback about how the request is progressing. The actionlib package provides -&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Tools to create servers that execute long-running goals that can be preempted.&lt;/li&gt;
  &lt;li&gt;Client interface to send requests to the server.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Take an example of an underwater vehicle which needs to reach a particular depth. Say from 5m to 10m. So with the DepthServer running in ROS Environment, any client can send
depth goals to the server stating that the vehicle needs to go to a depth of 10m.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://wiki.ros.org/actionlib?action=AttachFile&amp;amp;do=get&amp;amp;target=client_server_interaction.png&quot; alt=&quot;ROS Server&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;actionlib-message&quot;&gt;Actionlib Message:&lt;/h2&gt;

&lt;p&gt;The ActionClient and ActionServer communicate via a “ROS Action Protocol”. In order for the client and server to communicate, we need to define action specific messages.&lt;/p&gt;

&lt;p&gt;Message Format:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Define the goal
float32 depth_setpoint
---
# Define the result
float32 depth_final
---
# Define a feedback message
float32 depth_error
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;actionlib-cmake-settings&quot;&gt;Actionlib CMake Settings&lt;/h2&gt;
&lt;p&gt;These files are placed in a package’s ./action directory, lets call it DepthAction.action and the CMakeLists.txt must be updated as&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;find_package(catkin REQUIRED genmsg actionlib_msgs actionlib)
add_action_files(DIRECTORY action FILES Depth.action)
generate_messages(DEPENDENCIES actionlib_msgs)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Further package.xml must be updated as&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;build_depend&amp;gt;actionlib&amp;lt;/build_depend&amp;gt;
&amp;lt;build_depend&amp;gt;actionlib_msgs&amp;lt;/build_depend&amp;gt;
&amp;lt;run_depend&amp;gt;actionlib&amp;lt;/run_depend&amp;gt;
&amp;lt;run_depend&amp;gt;actionlib_msgs&amp;lt;/run_depend&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This would generate following messages:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;DepthAction.msg&lt;/li&gt;
  &lt;li&gt;DepthActionGoal.msg&lt;/li&gt;
  &lt;li&gt;DepthActionResult.msg&lt;/li&gt;
  &lt;li&gt;DepthActionFeedback.msg&lt;/li&gt;
  &lt;li&gt;DepthGoal.msg&lt;/li&gt;
  &lt;li&gt;DepthResult.msg&lt;/li&gt;
  &lt;li&gt;DepthFeedback.msg&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These messages are then used internally by actionlib to communicate between the ActionClient and ActionServer.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;actionclient&quot;&gt;ActionClient&lt;/h2&gt;
&lt;p&gt;Further we need to write ActionServer which keeps searching for ActionClients.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://wiki.ros.org/actionlib_tutorials/Tutorials/Writing%20a%20Simple%20Action%20Server%20using%20the%20Execute%20Callback%20%28Python%29&quot;&gt;ActionServer&lt;/a&gt; - Receives the goal from client, tracks its status, performs operation,  and return the results. It also checks if user requested preempts.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://wiki.ros.org/actionlib_tutorials/Tutorials/Writing%20a%20Simple%20Action%20Client%20%28Python%29&quot;&gt;ActionClient&lt;/a&gt; - Sends a goal to the ActionServer, can be used in GUIs and Mission Planners to send particular goals in manned and autonomous systems.&lt;br /&gt;
Here’s my implementation of DepthServer and DepthClient in Autonomous Underwater Vehicle -&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/actionlib/action.gif&quot; alt=&quot;Alpheus Depth Action Server&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 22 Nov 2017 23:29:00 +0530</pubDate>
        <link>http://localhost:4000/blog/2017/11/22/actionlib.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2017/11/22/actionlib.html</guid>
        
        <category>Mission Planner</category>
        
        <category>Actionlib</category>
        
        <category>SRMAUV</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Interactive Wall using Kinect and Servos</title>
        <description>&lt;p&gt;An Interactive wall is constructed of 40 servo motors embedded in the wall connected to a microcontroller and an RGB Camera with a depth sensor,
whole setup connected to a microprocessor. Object or human, if detected in the threshold using the RGB camera, servo in front of the object, gets initialized.
The servo updates with the change in the captured image creating a beautiful pattern. This was my first internship project at Proxbotics Creation Technologies.&lt;/p&gt;

&lt;!--more--&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;microsoft-kinect-sensor&quot;&gt;Microsoft Kinect Sensor&lt;/h4&gt;
&lt;p&gt;Kinect sensor features an RGB camera, a depth sensor consisting of an infrared laser projector and an infrared CMOS sensor, and a multi-array microphone.
It also contains a LED light, a three-axis accelerometer, and a small servo controlling the tilt of the device.
Kinect’s IR projector sends out a pattern of infrared light beams, which bounces on the objects and is captured by the standard CMOS image sensor.
This captured image is passed on to the onboard PrimeSense PS1080 chip to be translated into the VGA sized depth image.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;   
&lt;img src=&quot;/assets/images/interactivewall/kinect.png&quot; width=&quot;400&quot; height=&quot;200&quot; align=&quot;center&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;servo-motors&quot;&gt;Servo Motors&lt;/h4&gt;
&lt;p&gt;Servos are DC motors that include a feedback mechanism that allows you to keep track of the position of the motor at every moment.
The way you control a servo is by sending it pulses with a specific duration. Generally, the range of a servo goes from 1 millisecond
for 0 degrees to 2 milliseconds for 180 degrees. You need to refresh your control signals every 20ms because the servo expects to get an
update every 20ms, or 50 times per second.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/interactivewall/pwm.jpg&quot; width=&quot;600&quot; height=&quot;300&quot; align=&quot;center&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;16-channel-pwmservo-controller&quot;&gt;16 Channel PWM/Servo Controller&lt;/h4&gt;
&lt;p&gt;An i2c-controlled PWM driver with a built-in clock used to control 16 free-running PWM outputs! You can even chain up 62 breakouts to
control up to 992 PWM outputs. &lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;hardware-interfacing&quot;&gt;Hardware Interfacing&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Setting Up Raspberry Pi 3 for Kinect Support:   &lt;br /&gt;
Install Raspbian Jessie and boot-up Raspberry-Pi and follow the following steps to setup libfreenect open-source driver for Kinect interfacing –&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Installing Dependencies:
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get update
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get upgrade
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get install cmake libudev0 libudev-dev freeglut3 freeglut3-dev libxmu6 libxmu-dev libxi6 libxi-dev
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Setting Up libfreenect Driver
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone git://github.com/OpenKinect/libfreenect.git
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;libfreenect
mkdir build
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;build
cmake &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; ..
make
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;make install
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;ldconfig /usr/local/lib64/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;To use Kinect as a non-root user
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;adduser &lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt; video
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;adduser &lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt; plugdev
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;nano /etc/udev/rules.d/51-kinect.rules
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Paste the following lines in the newly created file
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# ATTR{product}==&quot;Xbox NUI Motor&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;SUBSYSTEM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;usb&quot;&lt;/span&gt;, ATTR&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;idVendor&lt;span class=&quot;o&quot;&gt;}==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;045e&quot;&lt;/span&gt;, ATTR&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;idProduct&lt;span class=&quot;o&quot;&gt;}==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;02b0&quot;&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;MODE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0666&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ATTR{product}==&quot;Xbox NUI Audio&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;SUBSYSTEM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;usb&quot;&lt;/span&gt;, ATTR&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;idVendor&lt;span class=&quot;o&quot;&gt;}==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;045e&quot;&lt;/span&gt;, ATTR&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;idProduct&lt;span class=&quot;o&quot;&gt;}==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;02ad&quot;&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;MODE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0666&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ATTR{product}==&quot;Xbox NUI Camera&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;SUBSYSTEM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;usb&quot;&lt;/span&gt;, ATTR&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;idVendor&lt;span class=&quot;o&quot;&gt;}==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;045e&quot;&lt;/span&gt;, ATTR&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;idProduct&lt;span class=&quot;o&quot;&gt;}==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;02ae&quot;&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;MODE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0666&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ATTR{product}==&quot;Xbox NUI Motor&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;SUBSYSTEM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;usb&quot;&lt;/span&gt;, ATTR&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;idVendor&lt;span class=&quot;o&quot;&gt;}==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;045e&quot;&lt;/span&gt;, ATTR&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;idProduct&lt;span class=&quot;o&quot;&gt;}==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;02c2&quot;&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;MODE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0666&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ATTR{product}==&quot;Xbox NUI Motor&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;SUBSYSTEM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;usb&quot;&lt;/span&gt;, ATTR&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;idVendor&lt;span class=&quot;o&quot;&gt;}==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;045e&quot;&lt;/span&gt;, ATTR&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;idProduct&lt;span class=&quot;o&quot;&gt;}==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;02be&quot;&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;MODE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0666&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ATTR{product}==&quot;Xbox NUI Motor&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;SUBSYSTEM&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;usb&quot;&lt;/span&gt;, ATTR&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;idVendor&lt;span class=&quot;o&quot;&gt;}==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;045e&quot;&lt;/span&gt;, ATTR&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;idProduct&lt;span class=&quot;o&quot;&gt;}==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;02bf&quot;&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;MODE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0666&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;LogOut and Log back in, open terminal and test with:
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;freenect-glview
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&quot;center&quot;&gt; Image View &lt;br /&gt;
&lt;img src=&quot;/assets/images/interactivewall/kinect2.jpeg&quot; width=&quot;500&quot; height=&quot;250&quot; align=&quot;center&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; Connections:  &lt;br /&gt;
&lt;img src=&quot;/assets/images/interactivewall/kinect.jpeg&quot; width=&quot;500&quot; height=&quot;200&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;control-code&quot;&gt;Control Code&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Approach 1 – Python + Raspberry Pi 3B + 16 Channel PWM Servo Driver&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Approach 2 – Processing IDE + Raspberry Pi 3B + 16 Channel PWM Servo Driver&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Approach 3 – Processing IDE + Raspberry Pi 3B + Arduino(Serial Comm.) + 16 Channel PWM Servo Driver&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Approach 4 – Processing IDE + Raspberry Pi 3b + Arduino Firmata + 16 Channel PWM Servo Driver&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 06 Jun 2017 13:06:00 +0530</pubDate>
        <link>http://localhost:4000/post/2017/06/06/interactive-wall.html</link>
        <guid isPermaLink="true">http://localhost:4000/post/2017/06/06/interactive-wall.html</guid>
        
        <category>Microsoft-Kinect</category>
        
        <category>Raspberry-Pi</category>
        
        <category>Arduino</category>
        
        <category>Servo</category>
        
        <category>ProcessingIDE</category>
        
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Underwater Object Detection and Tracking</title>
        <description>&lt;p&gt;Basic task of an Autonomous Underwater Vehicle is to detect an object underwater. Our problem statement included to detect a gate and an AUV should pass through the gate. Basic image processing techniques like RGB, HSV or Lab colorspace filteration didn’t work well even with histogram equlizations due to change in color of gate with different lightning conditions underwater. So I tried to implement Deep Learning to classify, detect and draw a bounding box over the pipes of the Gate.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;
&lt;p&gt;I tried using Convolutional Neural Networks for the task. Generally hugh data and training time is required to train CNNs from scratch to get perfect results which is not possible in the above task in which generating a large dataset containing Gate images is a hard job and traning on high end GPUs is very expensive. Workaround over this is Transfer Learning, in short - remove the last fully-connected layer of a pre-trained CNN (this layer’s outputs are the 1000 class scores for a different task like ImageNet), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset. Further fine-tune the weights of the pretrained network by continuing the backpropagation on the Gate’s dataset.
Amazing referece for Transfer Learning - &lt;a href=&quot;http://cs231n.github.io/transfer-learning/&quot;&gt;CS231n&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;TensorFlow Object Detection module provides trainable detection models in their &lt;a href=&quot;https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md&quot;&gt;model zoo&lt;/a&gt;. I chose Single Shot MultiBox Detectors(SSD) with MobileNet architecture.
Reason to choose the architecture - As our hardware is restricted to embedded systems, and dense deep learning models require high computational resources for inference, so i chose a comparatively less dense model which can infere a video feed to detect Gate on a Jetson TX1 with the other systems of the repository working parallely.
Reason to chose &lt;a href=&quot;https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab&quot;&gt;Single Shot MultiBox Detectors&lt;/a&gt; - Algorithm to draw bounding box on the object which needs to be detected.&lt;/p&gt;

&lt;h2 id=&quot;procedure&quot;&gt;Procedure&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Collected 140 images of gate from underwater recording, youtube videos.&lt;/li&gt;
  &lt;li&gt;Label the images using &lt;a href=&quot;https://github.com/tzutalin/labelImg&quot;&gt;LabelImg&lt;/a&gt; tool. It automatically generates the XML.&lt;/li&gt;
  &lt;li&gt;Split the data into test/train samples.&lt;/li&gt;
  &lt;li&gt;Generate TFRecords&lt;/li&gt;
  &lt;li&gt;Setup .config file for the SSD with custom hyperparameters&lt;/li&gt;
  &lt;li&gt;Export graph from trained model.&lt;/li&gt;
  &lt;li&gt;OpenCV optimization for inference on video feed.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/deep_data/1.jpg&quot; alt=&quot;1&quot; width=&quot;1000&quot; height=&quot;1000&quot; /&gt;
Labelling the dataset for Single Shot Detectors-&lt;br /&gt;
&lt;img src=&quot;/assets/images/deep_data/2.png&quot; alt=&quot;2&quot; width=&quot;1000&quot; height=&quot;1000&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;
&lt;p&gt;I trained the model on a Nvidia GTX 960m using CUDA. The tensorboard results -
&lt;img src=&quot;/assets/images/deep_data/3.png&quot; alt=&quot;3&quot; width=&quot;1000&quot; height=&quot;1000&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;inferece&quot;&gt;Inferece&lt;/h2&gt;
&lt;p&gt;The results on the test data-set were -
&lt;img src=&quot;/assets/images/deep_data/4.jpg&quot; alt=&quot;4&quot; width=&quot;1000&quot; height=&quot;1000&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 22 Apr 2017 04:59:00 +0530</pubDate>
        <link>http://localhost:4000/blog/2017/04/22/ssd.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2017/04/22/ssd.html</guid>
        
        <category>Image Processing</category>
        
        <category>Tensorflow</category>
        
        <category>Deep Learning</category>
        
        <category>SSD</category>
        
        <category>Object Classification</category>
        
        <category>Object Detection</category>
        
        <category>SRMAUV</category>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Propeller Clock (Presistence Of Vision Display)</title>
        <description>&lt;p&gt;POV display spins 360 degrees. The purpose of POV display project is to create a small apparatus that will create a visual using only a small number of LEDs as it spins in a circle. When the LEDs rotate several times around a point in less than a second, the human eye reaches its limit of motion perception and creates an illusion of a continuous image. I built this as a fun project in my 1st semester of college.&lt;br /&gt;
&lt;strong&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=eMwQxNTYRyg&quot;&gt;Video&lt;/a&gt; &lt;/strong&gt;
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;The pictures generated by the spinning LEDs are coordinated by an Arduino UNO microcontroller. A Hall Effect sensor is used in conjunction with a strong magnet so that the microcontroller can receive a reference point as to when it should start outputting the visual during each rotation. The RPM of motor, diameter of LED’s decides the POV phenomena.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Arduino UNO&lt;/li&gt;
  &lt;li&gt;Hall Effect Sensor&lt;/li&gt;
  &lt;li&gt;Magnet Piece&lt;/li&gt;
  &lt;li&gt;8 Single Color LEDs&lt;/li&gt;
  &lt;li&gt;9V Battery with Arduino Cap&lt;/li&gt;
  &lt;li&gt;12V 1000RPM motor&lt;/li&gt;
  &lt;li&gt;10k ohm resistor&lt;/li&gt;
  &lt;li&gt;8 * 330 ohm resistors&lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&quot;center&quot;&gt; Pinout: &lt;br /&gt;
&lt;img src=&quot;/assets/images/pov/arduino.png&quot; align=&quot;center&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; My Model: &lt;br /&gt;
&lt;img src=&quot;/assets/images/pov/model.jpg&quot; align=&quot;center&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; How are letters formed through LEDs? &lt;br /&gt;
&lt;img src=&quot;/assets/images/pov/letter.png&quot; align=&quot;center&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt; Click on the Image to see the Video: &lt;br /&gt;
&lt;a href=&quot;https://www.youtube.com/watch?v=eMwQxNTYRyg&quot;&gt;
&lt;img src=&quot;https://img.youtube.com/vi/eMwQxNTYRyg/hqdefault.jpg&quot; align=&quot;center&quot; /&gt;
&lt;/a&gt;
&lt;/p&gt;
</description>
        <pubDate>Sun, 09 Apr 2017 13:06:00 +0530</pubDate>
        <link>http://localhost:4000/post/2017/04/09/pov-display.html</link>
        <guid isPermaLink="true">http://localhost:4000/post/2017/04/09/pov-display.html</guid>
        
        <category>Presistence-Of-Vision</category>
        
        <category>Arduino</category>
        
        
        <category>post</category>
        
      </item>
    
      <item>
        <title>Proportional-Integral-Derivative Controller</title>
        <description>&lt;p&gt;Proportional-Integral-Derivative is a control loop feedback mechanism used in industrial control systems. I implemented it in Autonomous Underwater Vehicle, so here’s my intutition thorugh this blog post.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id=&quot;proportional-term&quot;&gt;Proportional Term:&lt;/h2&gt;

&lt;p&gt;Mr. Proportional : He looks at where the output is and compares to what you asked for. Hence it says:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;There is large error : Take Big Action&lt;/li&gt;
  &lt;li&gt;There is small error : Take Small Action&lt;/li&gt;
  &lt;li&gt;You have is what you asked for : Take No Action&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So basically what is this error?&lt;br /&gt;
Take a scenario of depth controller in an AUV, Pressure sensor returns the measured pressure value which is the value you have and now you want to reach a particular pressure value say from 520 to 530. So you’ll probablly have to initiate depth thrusters. The change in PWM of thruster servos must be controlled through a controller, here’s where the controller part comes in to reduce the error form 10 to 0.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pError = Setpoint - SensorValue;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;P&lt;sub&gt;out&lt;/sub&gt; = K&lt;sub&gt;p&lt;/sub&gt; * pError
K&lt;sub&gt;p&lt;/sub&gt; - Proportional Gain Constant.&lt;br /&gt;
Issue with P: Steady State Error.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;integral-term&quot;&gt;Integral Term:&lt;/h2&gt;

&lt;p&gt;Mr. Integral : He looks at the same error value, but compares it to how long its been that way and states:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;You have a chronic/acute error (Small errors for long time/ Big error or small time) - Take Big Action&lt;/li&gt;
  &lt;li&gt;You have mild error (Small error for short time) - Take Small Action&lt;/li&gt;
  &lt;li&gt;Your error history is neutral - Take No Action&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What is the use of Integral term?&lt;br /&gt;
Actually when ‘P’ mode alone is used, we would face the offset (+ve or -ve deviation from setpoint) problem.
Consider the case of heading controller in an AUV, Inertial Measurment Sensor would provide the heading angles, now say a setpoint to turn 30 degree, Proportional would take action generating an offset, making the vehicle oscillate around +ive and  -ive directions of setpoint. In order to nullify steady state error, ‘I’ mode is introduced, Integral action would add up all the error with respect to time and it will track the system to its setpoint.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;iError = iError + pError * dt;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I&lt;sub&gt;out&lt;/sub&gt; = K&lt;sub&gt;i&lt;/sub&gt; * &lt;script type=&quot;math/tex&quot;&gt;\int_0^t pError(t) \,dt&lt;/script&gt;&lt;br /&gt;
K&lt;sub&gt;i&lt;/sub&gt; = Integral Gain Constant&lt;/p&gt;

&lt;p&gt;Issue with PI: Overshooting-Integral Windup i.e When large change in setpoint occurs, Integral term accumulates a significant error during the rise, thus overshooting, making PI controller ineffective in dynamic conditions.&lt;br /&gt;
Amazing example of Integral Windup - &lt;a href=&quot;https://instrumentationtools.com/what-is-integral-wind-up/&quot;&gt;ClickMe&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;derivative-term&quot;&gt;Derivative Term:&lt;/h2&gt;

&lt;p&gt;Mr. Derivative: He also looks at the same error, but compares it to how its changing. He says:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Error is getting bigger: Take bigger action&lt;/li&gt;
  &lt;li&gt;Error is getting smaller: Take negative action&lt;/li&gt;
  &lt;li&gt;Error is not changing: Take no action&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What does Derivative Term work on?&lt;br /&gt;
While driving a car behind someone who is already at stop. As you get closer to them, not only you want to leave the accelerator, but also want to apply brake. This braking action is given by Derivative in PID Controller. It tells to slowdown more if you are getting closer to the target. Derivative control adds another dimension of complexity to control loops. It does have its benefits, but only in special cases.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dError = (pError - previousError);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;D&lt;sub&gt;out&lt;/sub&gt; = K&lt;sub&gt;d&lt;/sub&gt; * &lt;script type=&quot;math/tex&quot;&gt;\frac{d pError(t)}{dt}&lt;/script&gt;&lt;br /&gt;
K&lt;sub&gt;d&lt;/sub&gt; = Derivative Gain Constant&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Summing up the above three controllers:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ControlSignal = kp*pError + ki*iError + kd*dError;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;P&lt;sub&gt;out&lt;/sub&gt; + I&lt;sub&gt;out&lt;/sub&gt; + D&lt;sub&gt;out&lt;/sub&gt; =  K&lt;sub&gt;p&lt;/sub&gt; * pError + K&lt;sub&gt;i&lt;/sub&gt; * &lt;script type=&quot;math/tex&quot;&gt;\int_0^t pError(t) \,dt&lt;/script&gt; + K&lt;sub&gt;d&lt;/sub&gt; * &lt;script type=&quot;math/tex&quot;&gt;\frac{d pError(t)}{dt}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Further this ControlSignal is mapped to the control device output, i.e in the case of AUVs it is the thruster’s servo PWM values or in the case of quadcopters it is PWM to the ESCs.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;pid-constant-tuning&quot;&gt;PID Constant Tuning:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Set K&lt;sub&gt;i&lt;/sub&gt; and K&lt;sub&gt;d&lt;/sub&gt; = 0. Increase K&lt;sub&gt;p&lt;/sub&gt; until system oscillates.&lt;/li&gt;
  &lt;li&gt;Adjust K&lt;sub&gt;i&lt;/sub&gt; so oscillations stop.&lt;/li&gt;
  &lt;li&gt;Finally adjust K&lt;sub&gt;d&lt;/sub&gt; for fast response.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 23 Nov 2016 04:59:08 +0530</pubDate>
        <link>http://localhost:4000/blog/2016/11/23/pid.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2016/11/23/pid.html</guid>
        
        <category>SRMAUV</category>
        
        <category>Control Systems</category>
        
        <category>Algorithms</category>
        
        
        <category>blog</category>
        
      </item>
    
  </channel>
</rss>
